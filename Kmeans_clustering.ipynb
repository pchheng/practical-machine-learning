{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-means clustering  <img src=\"img/logo.png\",width=140,height=140, align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-means clustering is a simple clustering algorithm. In this case we face an unsupervised problem, meaning that we don't have classes or output labels for our data, but we want to cluster it in a way that data belonging to a cluster are similar to each other, and different from data belonging to other clusters.\n",
    "\n",
    "In this notebook we'll take you through an example of how this is done in practice. First we'll start by showing you some steps to take when first exploring your data. Then, we'll show the $k$-means clustering algorithm and will decide what's the best number of clusters to choose. Let's start by importing the necessary libraries. If you're not familiar with Jupyter notebooks: **Hold \"Shift\" and press \"Enter\" to execute a cell and move to the next cell.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.cluster.vq import kmeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this exercises we'll use a dataset containing information on wines. We want to discover clusters of different types of wine, and we have data on the wines' chemical characteristics. The data is the result of chemical analysis of wines grown in the same region in Tuscany, Italy but derived from different cultivars. The analysis determined the quantities of 13 constituents found in each wine. We want to see whether we can detect patterns in the data, such that we are able to cluster the wines together into distinct types. \n",
    "\n",
    "Our data consists of the following features:\n",
    "\n",
    "1. Alcohol \n",
    "2. Malic acid \n",
    "3. Ash \n",
    "4. Alcalinity of ash \n",
    "5. Magnesium \n",
    "6. Total phenols \n",
    "7. Flavanoids \n",
    "8. Nonflavanoid phenols \n",
    "9. Proanthocyanins \n",
    "10. Color intensity \n",
    "11. Hue \n",
    "12. OD280/OD315 of diluted wines \n",
    "13. Proline \n",
    "\n",
    "As this is a clustering exercises, we don't have any pre-determined classes to classify the wine, but we will try and detect these in our data. Let's get started!\n",
    "\n",
    "![alt text](img/wines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in our data. We'll use unlabeled wine data for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wine_data_nolabels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now stored in a [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) in the variable `df`. As you can see our data consists of chemical measures of each wine. How many different wines there are in total in our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring our data\n",
    "\n",
    "Alright, that's useful to know. So we're looking at 178 different wines, and we have 13 different features, i.e. chemical measurements on them. Before we dive into the clustering, let's do a little bit of exploration of our data to see whether there are things that look weird or suspicious. You could start by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, getting a massive table of summary statistics might seem like a good starting point, but it's not actually very informative just by glancing at it. It might be a better idea to create a few informative graphs. Before we do that, we want to transform our data into an NumPy array. \n",
    "\n",
    "Now hold on…why on earth do we want to convert our data to a NumPy array? Well, it turns out that the NumPy array data structure holds some benefits over Python lists and Pandas DataFrames, such as: being more compact, faster access in reading and writing items, being more convenient and more efficient. Moreover the NumPy library has a huge amount of high-level mathematical functions we can use on its arrays. So think of these arrays as a powerful data structure for efficient computation, which are excellent to use for things like machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is an easy way to do it, let's store our features in an array\n",
    "# called `X`.\n",
    "X = np.array(df).astype(float)\n",
    "\n",
    "# And we'll save those column names, they will come in handy later.\n",
    "header = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots \n",
    "\n",
    "In the following cell we'll create a bunch of plots. These are bi-variate scatterplots of the individual features in our data. We'll get a feel for how our features relate to each other, and we might be able to eye-ball correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 12))\n",
    "feat_comb_1 = [1, 2, 3, 4, 5]\n",
    "feat_comb_2 = [1, 2, 3, 4, 5]\n",
    "\n",
    "# This next line was alluded to above, it essentially gives the\n",
    "# transpose of `X`.\n",
    "feature_array = [X[:, j] for j in range(len(header))]\n",
    "\n",
    "nfeat = len(feat_comb_1)\n",
    "\n",
    "for j in range(nfeat):\n",
    "    for k in range(nfeat):\n",
    "        # `subplot` takes 3 arguments. If the final plot is going to\n",
    "        # be 4 subplots × 4 subplots for example, both of these arguments\n",
    "        # must be equal to 4. The third argument should be incremented\n",
    "        # sequentially and matplotlib will then decide, for example that\n",
    "        # in the case of a 5×5 matrix of plots, the 9th plot should be\n",
    "        # in the 4th plot on the second row.\n",
    "        plt.subplot(nfeat, nfeat, j + 1 + k * nfeat)\n",
    "        plt.scatter(feature_array[feat_comb_1[j]], feature_array[feat_comb_2[k]])\n",
    "        plt.xlabel(header[feat_comb_1[j]])\n",
    "        plt.ylabel(header[feat_comb_2[k]])\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you find that there are any particular patterns in our data?\n",
    "\n",
    "### Boxplots \n",
    "\n",
    "We can also make that `df.describe()` command a bit easier on the eye. You could for example plot box-plots of each feature in the same plot, so you can visually look at things like mean, outliers, distributions etc in one go. Let's give it a try below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 6))\n",
    "bp = plt.boxplot(X)\n",
    "plt.setp(bp['boxes'], color='black')\n",
    "plt.setp(bp['whiskers'], color='black')\n",
    "plt.setp(bp['fliers'], color='red', marker='o')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Value')\n",
    "axes.set_xticklabels(header, rotation=270)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uuuuh...right. This plot looks terrible and is not very informative. What is the problem here do you think? We'll address it below.\n",
    "\n",
    "### Correlations\n",
    "\n",
    "NumPy contains a `corrcoef` function which allows you to calculate the $n \\times n$ correlations matrix of an $n \\times m$ feature matrix. That means that you can look at all correlations between our features in one easy step, without having to eyeball it from scatterplots like we did before. Let's have a look at a selection of features stored in our NumPy array `X`. We'll calculate the correlation matrix, and plot the matrix as a heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = np.corrcoef(X, rowvar=0)\n",
    "\n",
    "# A nice way to visualise the correlations matrix is to make a\n",
    "# scatterplot and rather than write values, assign a colour map.\n",
    "plt.pcolor(correlation_matrix, cmap='hot', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "# Put the major ticks at the middle of each cell.\n",
    "plt.yticks(np.arange(0.5, 13.5), range(1, 14))\n",
    "plt.xticks(np.arange(0.5, 13.5), range(1, 14))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you understand how to interpret this plot?\n",
    "\n",
    "### Feature scaling\n",
    "\n",
    "Remember that ugly boxplot plot earlier? You might have understood that our features are all on different scales. This will become problematic when trying to cluster our data. Often when dealing with real data, it is wise to normalise it. For example, if we are looking at one feature whose mean is 100 and another whose mean is 0.0001, it is difficult to compare a measurement of these two features. \n",
    "\n",
    "More generally, feature scaling is pretty much always necessary if you want to run a complex machine learning algorithm. If we would ignore the different scales, the ML optimisation problem will take very long to solve. To make sure all our features are on the same scale, you could apply a function to feature scale our features. (You can also do this with scikit-learn, but it's good to understand how feature scaling works). \n",
    "\n",
    "A good function to scale features would be:\n",
    "\n",
    "$$x_i = \\frac{x_i - \\mu_i}{\\mathrm{std}(x_i)}$$\n",
    "\n",
    "You can use the `mean()` and `max()` functions built into Pandas to do this. The function should return both the scaled input vector, as well as the mean and standard deviation to be used to scale future samples.\n",
    " \n",
    "scikit-learn also contains a `preprocessing` module. This can transform the data, meaning we don't have to go through the arduous steps of calculating the mean and standard deviation and then applying the transformation manually. Whether you want to normalise in the sense that the mean in all features is zero, the variance is unity, or both, all of these options are available by setting the optional arguments: `with_mean=True/False` and  `with_std=True/False` as required. For more information, see the documentation at http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "Let's see how to do this for our data `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to check whether that actually worked, let's plot those boxplots again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 6))\n",
    "bp = plt.boxplot(X_scaled)\n",
    "plt.setp(bp['boxes'], color='black')\n",
    "plt.setp(bp['whiskers'], color='black')\n",
    "plt.setp(bp['fliers'], color='red', marker='o')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Value')\n",
    "axes.set_xticklabels(header, rotation=270)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK that's much nicer. Now that we've explored our data and have scaled our features, we're ready for some clustering...\n",
    "\n",
    "# $k$-means clustering \n",
    "\n",
    "The $k$-means clustering algorithm clusters data by trying to separate samples in $n$ groups of equal variance, minimising some measure of distance between the data points and cluster centroids. scikit-learn's implementation of $k$-means clustering basically calculates the \"sum of distances of samples to their closest cluster centre\" and calls this \"inertia\".\n",
    "\n",
    "This algorithm requires the number of clusters to be specified beforehand. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n",
    "\n",
    "The $k$-means clustering algorithm divides a set of $n$ samples $X$ into $k$ disjoint clusters $C$, each described by the mean $m_j$ of the samples in the cluster. The means are commonly called the cluster \"centroids\"; The $k$-means clustering algorithm aims to choose centroids that minimise the inertia (measure of distance), or within-cluster sum of squared criterion.\n",
    "\n",
    "You can read more about it here:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "or here: \n",
    "http://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "\n",
    "The $k$-means clustering algorithm adjusts the centroids until further progress cannot be made, i.e. the change in distortion since the last iteration is less than some threshold. Let's see how this is done in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=5, init='k-means++')\n",
    "# The fit method in scikit-learn tends to be the one that\n",
    "# takes an n×m array as its input.\n",
    "clf.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great that was easy, but how on earth do I interpret this? My $k$-means clustering algorithm creates a mapping from my datapoints to their closest cluster. But that still is hard to interpret. A way of trying to interpret $k$-means clustering is to visualise it in a graph, using the mapping for the different cluster colors. Here's an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = {'k_means_3': KMeans(n_clusters=3),\n",
    "              'k_means_8': KMeans(n_clusters=8),\n",
    "              'k_means_5': KMeans(n_clusters=5)}\n",
    "\n",
    "fignum = 1\n",
    "for name, est in estimators.items():\n",
    "    fig = plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "\n",
    "    plt.cla()\n",
    "    est.fit(X_scaled)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X_scaled[:, 10], X_scaled[:, 3], X_scaled[:, 5],\n",
    "               c=labels.astype(np.float))\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Color_intensity')\n",
    "    ax.set_ylabel('Ash')\n",
    "    ax.set_zlabel('Magnesium')\n",
    "    fignum += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see from these graphs what a good number of clusters is?\n",
    "\n",
    "Extra points for this **Question:** why do you think I chose Colour intensity, Ash and Magnesium to plot the figure? I could have chosen 3 other features. Perhaps go back to the data exploration part to find clues!\n",
    "\n",
    "You could try and play around changing the features I select for plotting with this line: \n",
    "\n",
    "```python\n",
    "ax.scatter(X_scaled[:, 10], X_scaled[:, 3], X_scaled[:, 5], c=labels.astype(np.float))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising your algorithm; the good, the bad and the ugly\n",
    "\n",
    "You might have noticed that we set a value for the `init` argument before. This was to make you aware of this. Remember, we first wrote `clf = KMeans(n_clusters = 3, init='k-means++')`.\n",
    "\n",
    "The value for init, can be `k-means++`, `random` or an [`numpy.ndarray`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html). It's bascially the settings for initialisation of the algorithm, and it defaults to `k-means++`. This is a pretty important part of the algorithm in fact. \n",
    "\n",
    "Given enough time, $k$-means will always converge, however this may be to a local minimum. This is highly dependent on the initialisation of the centroids and the way we set it. As a result, it's best to do the computation often and several times, with different initialisations of the centroids. One method to help address this issue is the `k-means++` initialisation scheme, which has been implemented in scikit-learn (use the `init='kmeans++'` parameter). This initialises the centroids to be (generally) distant from each other, leading to provably better results than random initialisation.\n",
    "\n",
    "The way the documentation describes it is:\n",
    "\n",
    "* `k-means++`: selects initial cluster centres for $k$-means clustering in a smart way to speed up convergence.\n",
    "* `random`: choose $k$ observations (rows) at random from data for the initial centroids.\n",
    "\n",
    "The number of times the $k$-means algorithm will be run with different centroid seeds is refered to as `n_init`. The default value is 10, and as we said before it's best to do it a couple of times. With multiple runs, the final results will be the best output of `n_init` consecutive runs in terms of inertia (measure of distance). In practice, the $k$-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That's why it can be useful to restart it several times.\n",
    "\n",
    "Here are some examples for good and bad practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bad\n",
    "KMeans(n_clusters=3, n_init=1, init='random')\n",
    "\n",
    "# Good\n",
    "KMeans(n_clusters=3, init='k-means++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What's bad about the first set-up? Why is the second one better? What about the default values, are they any good? (If you can't answer this go back and read the section above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-means clustering for large datasets\n",
    "\n",
    "When you're doing $k$-means on a very large dataset, the algorithm can sometimes be somewhat slow. A way to solve this is to use a method called mini-batch $k$-means clustering.\n",
    "\n",
    "Mini-batch $k$-means clustering is an alternative implementation that does incremental updates of the centroid positions using mini-batches. For large scale learning (say $n \\gt 10,0000$) mini-batch $k$-means is probably much faster than the default batch implementation.\n",
    "\n",
    "See also: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MiniBatchKMeans(n_clusters=3, init='k-means++')\n",
    "clf.fit(X_scaled)\n",
    "labels = clf.labels_\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "ax.scatter(X_scaled[:, 10], X_scaled[:, 3], X_scaled[:, 5],\n",
    "           c=labels.astype(np.float))\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Color_intensity')\n",
    "ax.set_ylabel('Ash')\n",
    "ax.set_zlabel('Magnesium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, it gives the same results but on large data this is generally much quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Curves and Choosing $k$\n",
    "\n",
    "How can we choose the right number of clusters $k$?\n",
    "\n",
    "Ideally, we want to have the smallest number of clusters that is able to separate our data in groups that are \"distinct enough\".\n",
    "\n",
    "Thus let us try all $0 \\lt k \\lt 10$, calculate the positions of all of the centroids, and then take the average of the Euclidean distance from every point to its closest centroid and plot this against $k$. \n",
    "\n",
    "For this exercise we'll use a slightly different way of calculating $k$-means, not using scikit-learn but SciPy instead. This is to show you there are more ways to implement this algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = range(1, 10)\n",
    "\n",
    "# Note here we are using a different library. Previously it was\n",
    "# scikit-learn, no we've moved to SciPy (see the top of the notebook\n",
    "# where we imported `scipy.cluster.vq.kmeans` and\n",
    "# `sklearn.cluster.KMeans`).\n",
    "\n",
    "KM = [kmeans(X_scaled, k) for k in K] # Apply kmeans 1 to 10.\n",
    "\n",
    "# The SciPy method stores the result in a funny format, it stores the\n",
    "# coordinates of all centroids and then the measure of average distance\n",
    "# of all points from their closest centroid (which is what we're after).\n",
    "distances = [KM[i][1] for i in range(len(KM))]\n",
    "\n",
    "plt.plot(range(1, 10), distances)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for k-means clustering') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a slightly less \"black box\" way of doing this, we can run a loop over all desired values of $k$, fitting the model (using scikit-learn this time) for each $k$ seperately. We then calculate the \"labels\" for each data point (i.e. which cluster it belongs to or which centroid it is closest to). Then we compute the Euclidean distance from each point to its closest centroid and take the average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = [[] for i in range(1,10)]\n",
    "euclidean_distances = [0 for i in range(len(X_scaled))]\n",
    "cluster_averaged_distances = [0 for i in range(1, 10)]\n",
    "\n",
    "for K in range(1, 10):\n",
    "    clf = KMeans(n_clusters=K, init='k-means++')\n",
    "    clf.fit(X_scaled.astype(float))\n",
    "    labels = clf.predict(X_scaled)\n",
    "    \n",
    "    centroids = clf.cluster_centers_\n",
    "    \n",
    "    for j in range(len(X_scaled)):\n",
    "        euclidean_distances[j] = 0\n",
    "        cluster = labels[j]\n",
    "        for k in range(3):\n",
    "            euclidean_distances[j] += (X_scaled[j][k] - centroids[cluster][k])**2\n",
    "        euclidean_distances[j] = (euclidean_distances[j])**0.5\n",
    "\n",
    "    cluster_averaged_distances[K - 1] = np.mean(euclidean_distances)\n",
    "               \n",
    "plt.plot(range(1, 10), cluster_averaged_distances)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for k-means clustering')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is substantially slower than using SciPy because running all of these loops isn't well optimised. scikit-learn has a similar method for calculating the \"Sum of distances of samples to their closest cluster center\", but it calculates the sum of the squared Euclidean distances, rather than the sum of the roots, meaning the answer is somewhat different than that obtained using SciPy (if we take the square root at the end, it brings us to the same ballpark but $\\Sigma_{i}\\sqrt{x_{i}}\\neq \\sqrt{\\Sigma_{i}x_{i}}$). Nonetheless, this is a completely valid metric, it's up to you which you'd prefer to use.\n",
    "\n",
    "Lastly, you can also calculate a different distance metric than Euclidian distance. Why should you care about different distance metrics? Well, your results and choice of $k$ can actually depend on the distance metric you chose. Here is an example of using the squared distance to create the same graph as above. Do you notice how things look different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squared_distances = []\n",
    "for k in range(1, 10):\n",
    "    clf = KMeans(n_clusters=k, init='k-means++')\n",
    "    clf.fit(X_scaled.astype(float))\n",
    "    squared_distances.append((clf.inertia_ / len(X_scaled))**0.5)\n",
    "    \n",
    "plt.plot(range(1, 10), squared_distances)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for k-means clustering')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Based on the three plots you've seen, what would your choice of $k$ be for this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copyright © ASI 2017 All rights reserved"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
