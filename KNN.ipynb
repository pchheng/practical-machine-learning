{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-nearest Neighbours <img src=\"img/logo.png\",width=140,height=140, align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-nearest neighbours (kNN) is a simple classification algorithm. It classifies cases based on a similarity measure relying on the labels belonging to the $k$ nearest points in the training set. In this notebook, we will show you how to do it with scikit-Learn, and in the appendix we'll show you how to do it from scratch. Let's start with importing the required libraries for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're already familiar with the dataset we use for this exercise, as we'll use the wine data again. This time, we have added the labels to the wines. The wines comes from the same region in Tuscany, Italy but are derived from three different cultivars. Did you discover 3 clusters in the last exercise? We're still working with the chemical features of the data, but we've added classes.\n",
    "\n",
    "The classes are given under the variables \"Class\" and \"Origin\" and are respectively:\n",
    "\n",
    "1. from Siena\n",
    "2. from Lucca\n",
    "3. from Pisa\n",
    "\n",
    "With our classficiaton algorithm we can predict where new, unseen wines are coming from. Let's start by reading in the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/wine_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great, so it looks like we've just added two columns to our dataset. Let's have a look at how our classes are divided. This is always important to do in classification problems. We would ideally like each of our classes to be quite equal in size, otherwise we would have to address a problem of unbalanced classes. So let's create a bar plot of our classes. We'll take the variable `Origin` but we could do the same with `Class`, as they're just the numeric and string representation of our classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine_origin = data['Origin'].value_counts()\n",
    "plt.figure()\n",
    "wine_origin.plot(kind='bar')\n",
    "plt.xlabel('Origin of wine')\n",
    "plt.ylabel('Number of instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So...do you think these classes are unbalanced?\n",
    "\n",
    "### Cross validation - splitting the data \n",
    "\n",
    "Now to test the algorithm, we first need to split the data into a training set and a test set, and convert the two sets to NumPy arrays. We'll use the `train_test_split` module in scikit-learn for this. We'll then have to split the classes out of the data to store it in a separate `Y` variable. We can use `Class` for `train_Y` and the rest of the variables, without `Origin` for `train_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, train_size=0.7)\n",
    "\n",
    "train_X = np.array(train)[:, 2:].astype(float)\n",
    "train_Y = np.array(train)[:, 0].astype(float)\n",
    "\n",
    "test_X = np.array(test)[:, 2:].astype(float)\n",
    "test_Y = np.array(test)[:, 0].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that our data is split into train and test data, let's run the kNN algorithm on our data using the scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 1\n",
    "clf = KNeighborsClassifier(K)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "predictions = clf.predict(test_X)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(test_Y, predictions) * 100\n",
    "print('Accuracy: {:.1f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the values in the data, we can see that they have different orders of magnitude for different features. Let's work our feature scaling magic and do a short exercise to scale our `train_X` set to make the different features better aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Can you scale the `train_X` feature set and `test_X` set using the `StandarScaler` function, like we did before in the $k$-means clustering exercise? Hint: if you get stuck, the answer is at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler1  = # Write here\n",
    "scaler2  = # Write here\n",
    "\n",
    "train_Xscaled = # Write here\n",
    "test_Xscaled= # Write here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's put the scaled data into the kNN algorithm and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 1\n",
    "clf = KNeighborsClassifier(K)\n",
    "clf.fit(train_Xscaled, train_Y)\n",
    "\n",
    "predictions = clf.predict(test_Xscaled)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(test_Y, predictions) * 100\n",
    "print('Accuracy: {:.1f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We can also investigate other metrics, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(test_Y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question:** Do you understand what these metrics mean? Is our model any good at predicting where wines come from? How is the fact that our dataset is relatively small weighing on these results?\n",
    "\n",
    "We can also try setting weights (i.e. using the distance of neighbours to weigh their relative importance), and see if our performance increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 1\n",
    "clf = KNeighborsClassifier(K, weights='distance')\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "predictions = clf.predict(test_X)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(test_Y, predictions) * 100\n",
    "print('Accuracy: {:.1f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we increase the number of neighbours taken into account? We can plot the accuracy accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_vector(train_X, train_Y, test_X, test_Y, weights, upperLim = 100):\n",
    "    results = []\n",
    "    for k in range(1, upperLim, 4):\n",
    "        clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors = k, weights = weights)\n",
    "        clf = clf.fit(train_X, train_Y)\n",
    "        preds = clf.predict(test_X)\n",
    "        accuracy = clf.score(test_X, test_Y)\n",
    "        results.append([k, accuracy*100])\n",
    " \n",
    "    results = np.array(results)\n",
    "    return(results)\n",
    "\n",
    "plt_vector1 = plot_vector(train_X, train_Y, test_X, test_Y, weights='uniform')\n",
    "plt_vector2 = plot_vector(train_X, train_Y, test_X, test_Y, weights='distance')\n",
    "plt.plot(plt_vector1[:, 0], plt_vector1[:, 1], label='uniform')\n",
    "plt.plot(plt_vector2[:, 0], plt_vector2[:, 1], label='distance')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(60, 80)\n",
    "plt.title('Accuracy with increasing $k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks quite funky, but that's again mostly because we have a very small dataset. Our results tend to jump around quite a bit because of it. Also, having a very high number of $k$ on a very small dataset makes very little sense..\n",
    "\n",
    "We can also do a step of feature selection, in order to maintain only the most descriptive features. More specifically, the `sklearn.feature_selection` module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators' accuracy scores or to boost their performance on very high-dimensional datasets. Univariate feature selection works by selecting the best features based on univariate statistical tests.\n",
    "\n",
    "First, we select the optimal number of features, through cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentiles = range(1, 100, 5)\n",
    "results = []\n",
    "\n",
    "for i in range(1, 100, 5):\n",
    "    fs = feature_selection.SelectPercentile(\n",
    "        feature_selection.f_classif, percentile=i)\n",
    "\n",
    "    X_train_fs = fs.fit_transform(train_Xscaled, train_Y)\n",
    "\n",
    "    scores = sklearn.model_selection.cross_val_score(\n",
    "        clf, X_train_fs, train_Y, cv=5)\n",
    "\n",
    "    results = np.append(results, scores.mean())\n",
    "\n",
    "optimal_percentile = np.where(results == results.max())[0]\n",
    "\n",
    "if len(optimal_percentile) > 1:\n",
    "    optimal_percentile = optimal_percentile[0]\n",
    "\n",
    "print('Optimal percentil: {}'.format(optimal_percentile))\n",
    "\n",
    "# Plot number of features vs. cross-validation scores.\n",
    "plt.figure()\n",
    "plt.xlabel('Number of features selected [%]')\n",
    "plt.ylabel('Cross validation accuracy')\n",
    "plt.plot(percentiles, results)\n",
    "print('Mean scores: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we select the relevant features and we repeat the kNN algorithm with the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs = sklearn.feature_selection.SelectPercentile(\n",
    "    feature_selection.f_classif,\n",
    "    percentile=percentiles[optimal_percentile])\n",
    "X_train_fs = fs.fit_transform(train_Xscaled, train_Y)\n",
    "\n",
    "clf = KNeighborsClassifier(5)\n",
    "\n",
    "clf.fit(X_train_fs, train_Y)\n",
    "X_test_fs = fs.transform(test_Xscaled)\n",
    "predictions = clf.predict(X_test_fs)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(test_Y, predictions) * 100\n",
    "print('Accuracy: {:.1f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to accuracy if we change the ratio between training and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Designing the kNN algorithm yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the kNN algorithm is very straightforward, you can easily set it up yourself. We would need to define all the functions we need to implement kNN. We'll do that below, to show you step by step how the kNN algorithm works.\n",
    "\n",
    "Let's start by calculating the distance between two data instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Euclidean Distance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(instance1, instance2):\n",
    "    length = len(instance1)\n",
    "    # You can also check if instance1 and instance2 have the same length.\n",
    "    distance = 0\n",
    "    for l in range(length):\n",
    "        distance += (instance1[l] - instance2[l])**2\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = [0, 1, 2]\n",
    "data2 = [0, 2, 4]\n",
    "distance = euclidean_distance(data1, data2)\n",
    "print('Distance: {:.2f}'.format(distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to get the $k$ nearest neighbors of a point in a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_neighbours(data, labels, test_instance, K):\n",
    "    distances = []\n",
    "    neighbours = {}\n",
    "    # Find the distances between all the points and create a list of tuples.\n",
    "    for i in range(len(data)):\n",
    "        dist = euclidean_distance(test_instance, data[i, :])\n",
    "        distances.append([data[i, :], dist])\n",
    "\n",
    "    # Sort the list of distances by using the second element of the tuple,\n",
    "    # i.e. the distance.\n",
    "    idx = np.argsort(np.array(distances)[:, 1])\n",
    "    neighbours_data = data[idx]\n",
    "    neighbours_label = labels[idx]\n",
    "    \n",
    "    neighbours = {'data': neighbours_data[:K], 'labels': neighbours_label[:K]}\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the training set: 2 points and 2 labels.\n",
    "data = np.array([[2, 2, 2], [4, 4, 4]])\n",
    "labels = np.array([0, 1])\n",
    "\n",
    "# Define the test instance\n",
    "test_instance = [5, 5, 5]\n",
    "\n",
    "# Choose the number of neighbours.\n",
    "K = 1\n",
    "\n",
    "# Find and retrieve the k nearest points to the test instance, sorted by the\n",
    "# distance.\n",
    "neighbours = get_neighbours(data, labels, test_instance, K)\n",
    "print(neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a response function: this counts the number of times a certain class appears in the set of neighbours that we've found with the previous function. The class with the highest frequency will be the label assigned to the test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_response(neighbours):\n",
    "    class_votes = {}\n",
    "    # Assign the votes for every class.\n",
    "    for i in range(len(neighbours)):\n",
    "        response = neighbours[i]\n",
    "        if response in class_votes:\n",
    "            class_votes[response] += 1\n",
    "        else:\n",
    "            class_votes[response] = 1\n",
    "    \n",
    "    # Use the dictionary to short which class has the most votes.\n",
    "    sorted_votes = sorted(class_votes.items(), key=operator.itemgetter(1),\n",
    "                          reverse=True)\n",
    "    return sorted_votes[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this case we have two 1s and one 0: class 1 wins.\n",
    "neighbours['labels'] = np.array([1, 1, 0])\n",
    "response = get_response(neighbours['labels'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the accuracy of our model: this should be a familiar concept by now, it is a way to test the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_accuracy(test_set, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(test_set)):\n",
    "        # If the label of the test_set and the prediction are the same add one.\n",
    "        if test_set[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (float(correct) / float(len(test_set))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# True labels.\n",
    "test_set = np.array(['a','a','b'])\n",
    "\n",
    "# Predicted labels.\n",
    "predictions = ['a', 'a', 'a']\n",
    "\n",
    "accuracy = get_accuracy(test_set, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Assign a label to the test instance, basing on the following training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = np.array([[1, 1, 1], [1, 3, 5], [7, 5, 4], [9, 5, 3]])\n",
    "training_labels = np.array([1, 2, 1, 2])\n",
    "test_instance = np.array([4, 4, 4])\n",
    "\n",
    "# Get K neighbours.\n",
    "K = # Type here\n",
    "neighbours = # Type here\n",
    "\n",
    "# Get the label\n",
    "label = # Type here\n",
    "\n",
    "print(label)\n",
    "\n",
    "# What about the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to the Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Can you scale the `train_X` feature set and `test_X` set using the `StandarScaler` function, like we did before in the K-means exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler1 = sklearn.preprocessing.StandardScaler().fit(train_X)\n",
    "scaler2 = sklearn.preprocessing.StandardScaler().fit(test_X)\n",
    "\n",
    "train_Xscaled = scaler1.transform(train_X)\n",
    "test_Xscaled = scaler2.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Assign a label to the test instance, basing on the following training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get k neighbours.\n",
    "K = 1\n",
    "neighbours = get_neighbours(training_set, training_labels, test_instance, K)\n",
    "\n",
    "# get the label\n",
    "label = get_response(neighbours['labels'])\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copyright © ASI 2017 All rights reserved"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
